# Hardening Deployment Summary - 2025-11-02

## ðŸŽ¯ **Deployment Complete - Production Ready**

All hardening updates have been deployed successfully. The monitoring stack now has bullet-proof math, non-flappy alerts, and comprehensive runbooks.

---

## âœ… **Changes Deployed**

### 1. Recording Rules (3 new rules)
**File**: `monitoring/prometheus-crm-events-rules.yml`
**Group**: `crm_events_lag_capacity`

```yaml
# Partition lag (source of truth for skew calculations)
- record: kafka:group_partition_lag
  expr: max by (topic, consumergroup, partition) (kafka_consumergroup_lag)

# Skew ratio per group (max/avg), safe-divide with clamp_min to prevent NaN
- record: kafka:group_skew_ratio
  expr: |
    (
      max by (topic, consumergroup) (kafka:group_partition_lag)
    ) / clamp_min(
      avg by (topic, consumergroup) (kafka:group_partition_lag),
      0.001
    )

# Hottest partition id (for runbooks & annotations)
- record: kafka:group_hottest_partition
  expr: topk(1, sum by (topic, consumergroup, partition) (kafka:group_partition_lag))
```

**Benefits**:
- âœ… No divide-by-zero errors (clamp_min ensures denominator â‰¥0.001)
- âœ… No NaN values from empty partitions
- âœ… Eliminates duplicate PromQL calculations
- âœ… Consistent results across panels and alerts

---

### 2. Alert Polish (2 alerts updated)

#### **CrmEventsHotKeySkewHigh**
**Changes**:
- âœ… Now uses `kafka:group_skew_ratio` recording rule (was raw calculation)
- âœ… Stability window increased: **12m** (was 10m) - reduces flapping during rebalances
- âœ… Enhanced annotations with actionable steps
- âœ… Includes runbook reference

**New Expression**:
```yaml
expr: kafka:group_skew_ratio{consumergroup="crm-events-sse"} > 4
for: 12m
```

**Annotations**:
```yaml
summary: "Kafka hot-key skew high (>{{ $value | humanize }}x)"
description: |
  Skew ratio (max/avg) is >4x for 12m. Likely key hot-spot.
  Current skew: {{ $value | humanize }}x
  Next steps: scale consumers to 2+, check partition key distribution.
  Runbook: monitoring/docs/RUNBOOK_HOTKEY_SKEW.md
```

#### **CrmEventsUnderReplicatedConsumers**
**Changes**:
- âœ… Stability window increased: **7m** (was 5m) - avoids noise during container restarts
- âœ… Severity: **info** (gentle warning, not urgent)
- âœ… Improved annotations with scaling commands

**New Expression**:
```yaml
expr: count(count by (memberid) (kafka_consumergroup_current_offset{consumergroup="crm-events-sse"})) < 2
for: 7m
```

---

### 3. Panel 19 Optimization
**File**: `monitoring/grafana/dashboards/crm_events_pipeline.json`

**Before** (raw calculation):
```json
"expr": "topk(1, sum by (partition) (kafka_consumergroup_lag{consumergroup=\"crm-events-sse\"})) / clamp_min(avg(sum by (partition) (kafka_consumergroup_lag{consumergroup=\"crm-events-sse\"})), 0.001)"
```

**After** (recording rule):
```json
"expr": "kafka:group_skew_ratio{consumergroup=\"crm-events-sse\"}"
```

**Benefits**:
- âœ… Cleaner query (single metric lookup)
- âœ… Consistent with alert logic
- âœ… Faster evaluation (pre-computed)
- âœ… Easier to troubleshoot

---

### 4. Documentation (2 new files)

#### **RUNBOOK_HOTKEY_SKEW.md** (Comprehensive Incident Playbook)
**Location**: `monitoring/docs/RUNBOOK_HOTKEY_SKEW.md`
**Size**: 400+ lines

**Sections**:
- âœ… Alert trigger definition
- âœ… 60-second triage checklist
- âœ… Diagnosis commands (PowerShell)
- âœ… 4 remediation options (scale, rebalance, key strategy, partitions)
- âœ… Verification criteria (3-5 minutes)
- âœ… Escalation procedures
- âœ… Prevention strategies (short-term + long-term)
- âœ… Related alerts reference

#### **ONE_LINER_FIXES.md** (Quick Reference Commands)
**Location**: `monitoring/docs/ONE_LINER_FIXES.md`
**Size**: 300+ lines

**Sections**:
- âœ… Instant remediation commands (scale, rebalance, drain boost)
- âœ… Monitoring loops (watch skew live, watch partition lag)
- âœ… Validation commands (recording rules, alerts, metrics)
- âœ… Troubleshooting commands (logs, health, resources)
- âœ… Emergency rollback procedures
- âœ… Performance tuning (partitions, message sizes)
- âœ… Cheat sheet table (problem â†’ command â†’ panel)

---

## ðŸ“Š **Current State**

### Partition Lag Distribution
```
Partition 0: 0 lag (balanced)
Partition 1: 0 lag (balanced)
Partition 2: 339 lag (hot partition from drill)
```

### Consumer Status
```
Active Members: 1 (single replica)
Panel 18: YELLOW (needs 2+ for green)
```

### Skew Status
```
Skew Ratio: >4x (339:0 distribution)
Panel 19: RED zone (expected)
Alert: CrmEventsHotKeySkewHigh will fire in ~12m
```

---

## ðŸš€ **Immediate Next Steps**

### 1. Scale to 2 Replicas (Recommended)
```powershell
docker compose up -d --scale crm-events=2
```

**Why**:
- Resolves `CrmEventsUnderReplicatedConsumers` alert (will fire in 7m)
- Provides failover capacity
- Doubles drain rate for partition 2
- Turns Panel 18 GREEN

**Expected Timeline**:
- 0s: Command executes, new container starting
- 10-15s: Consumer joins group, triggers rebalance
- 30s: Partitions redistributed (likely 2 partitions per consumer with 3 total)
- 60-90s: Lag starts draining
- 3-5min: Partition 2 lag reaches 0
- 12m: Skew ratio drops below 4x, alert auto-resolves

---

### 2. Monitor Via Dashboard
**URL**: http://localhost:3000/d/crm-events-pipeline

**Panels to Watch**:
- **Panel 17** (Per-Partition Lag Table): Watch partition 2 lag decrease
- **Panel 18** (Consumer Group Size): Should turn GREEN (value=2)
- **Panel 19** (Hot-Key Skew Ratio): Should drop from RED to GREEN (<2x)
- **Panel 13** (Consumer Lag Sum): Should trend downward

---

### 3. Verify Recording Rules (Optional)
```powershell
# Wait 30 seconds after Prometheus restart for rules to evaluate
Start-Sleep -Seconds 30

# Query skew ratio
curl.exe -s "http://localhost:9090/api/v1/query?query=kafka:group_skew_ratio{consumergroup='crm-events-sse'}"

# If empty result: Rules need more time or lag is zero (skew undefined with 0 lag)
```

---

## ðŸ“ˆ **System Metrics**

### Total Configuration
- **Dashboard Panels**: 19 (15 core + 2 bonus + 2 polish)
- **Recording Rules**: 7 (4 lag/capacity + 3 skew/partition)
- **Alert Rules**: 12 (3 lag + 2 hardening + 5 health + 2 SLO)
- **Documentation**: 5 files (2,800+ lines)

### Monitoring Stack
- **Prometheus**: v2.54.1 (scraping every 15s, evaluating rules every 15-30s)
- **Grafana**: v11.2.0 (auto-provisioning dashboard on startup)
- **Kafka Exporter**: v1.7.0 (exposing lag metrics on port 9308)
- **Redpanda/Kafka**: v24.2.4 (3 partitions, consumer group "crm-events-sse")

---

## ðŸ” **Validation Checklist**

### Recording Rules
- [x] `kafka:group_partition_lag` - Aggregates per-partition lag
- [x] `kafka:group_skew_ratio` - Calculates max/avg with safe division
- [x] `kafka:group_hottest_partition` - Identifies worst partition

### Alerts
- [x] `CrmEventsHotKeySkewHigh` - Uses recording rule, 12m window
- [x] `CrmEventsUnderReplicatedConsumers` - 7m window, info severity

### Panels
- [x] Panel 18 - Consumer Group Size (thresholds: 0=RED, 1=YELLOW, 2+=GREEN)
- [x] Panel 19 - Hot-Key Skew Ratio (uses recording rule, thresholds: <2=GREEN, 4+=RED)

### Documentation
- [x] RUNBOOK_HOTKEY_SKEW.md - Comprehensive incident playbook
- [x] ONE_LINER_FIXES.md - Quick reference commands
- [x] HARDENING_DEPLOYMENT_SUMMARY.md - This file

---

## ðŸŽ“ **Key Learnings**

### Bullet-Proof Math
- Always use `clamp_min()` for denominators in division (prevents divide-by-zero)
- Use `by (topic, consumergroup)` label grouping to ensure consistent aggregation
- Recording rules with safe math eliminate NaN/Inf values in dashboards

### Non-Flappy Alerts
- Increase `for:` duration after scaling from 10m â†’ 12m (allows rebalancing)
- Use `info` severity for gentle warnings (under-replication vs critical outage)
- Include actionable steps in annotations (command to run, not just "fix it")

### Panel Optimization
- Recording rules reduce duplicate PromQL calculations
- Cleaner queries are easier to troubleshoot
- Consistent metrics across panels and alerts prevent confusion

---

## ðŸ“š **References**

### Access URLs
- **Dashboard**: http://localhost:3000/d/crm-events-pipeline
- **Prometheus Alerts**: http://localhost:9090/alerts
- **Prometheus Rules**: http://localhost:9090/rules
- **Kafka Metrics**: http://localhost:9308/metrics
- **CRM Events Service**: http://localhost:9010/metrics

### Files Modified
1. `monitoring/prometheus-crm-events-rules.yml` - Added 3 recording rules, updated 2 alerts
2. `monitoring/grafana/dashboards/crm_events_pipeline.json` - Updated Panel 19 query

### Files Created
1. `monitoring/docs/RUNBOOK_HOTKEY_SKEW.md` - Incident playbook (400+ lines)
2. `monitoring/docs/ONE_LINER_FIXES.md` - Quick commands (300+ lines)
3. `monitoring/docs/HARDENING_DEPLOYMENT_SUMMARY.md` - This file

### Related Documentation
- `monitoring/docs/LAG_CAPACITY_MONITORING.md` - Architecture & implementation
- `monitoring/docs/QUICK_VALIDATION.md` - Validation checklist
- `monitoring/docs/AUTOPROV_COMPLETE.md` - Auto-provisioning guide

---

## âœ¨ **What's Next?**

### Immediate Actions
1. **Scale consumers**: `docker compose up -d --scale crm-events=2`
2. **Watch dashboard**: Monitor Panel 17, 18, 19 for green status
3. **Wait for drain**: 3-5 minutes to clear 339 lag backlog

### Optional Enhancements (Future)
- **Grafana Annotations**: Add Prometheus alert annotations to show alert markers on graphs
- **Auto-Scaling**: Wire up auto-scaler based on `kafka:group_lag_sum` threshold
- **/ready Probe**: Create HTTP health endpoint that fails when skew >4x or consumers <2
- **Partition Increase**: If hot-key persists, increase partitions from 3 â†’ 6
- **Alert Inhibition**: Configure Alertmanager to inhibit `UnderReplicated` when `ServiceDown` fires

### Monitoring Best Practices
- âœ… Run 2 consumer replicas as baseline (current: 1)
- âœ… Monitor Panel 19 weekly for trending skew
- âœ… Review RUNBOOK_HOTKEY_SKEW.md quarterly
- âœ… Consider composite keys if hot-key becomes chronic

---

## ðŸ† **Success Criteria**

### Deployment Success (Current)
- âœ… 3 recording rules loaded in Prometheus
- âœ… 2 alerts updated with stability windows
- âœ… Panel 19 using optimized query
- âœ… 2 runbooks created and accessible
- âœ… Prometheus & Grafana restarted successfully

### Operational Success (After Scaling)
- â³ Panel 18: GREEN (2 consumers)
- â³ Panel 19: GREEN (skew <2x)
- â³ Partition 2 lag: 0 (drained)
- â³ Alert: `CrmEventsHotKeySkewHigh` auto-resolved
- â³ Alert: `CrmEventsUnderReplicatedConsumers` auto-resolved

---

**Deployment Date**: 2025-11-02
**Deployed By**: DevOps Team
**Status**: âœ… **PRODUCTION READY**
**Next Review**: After scaling to 2 consumers and verifying drain

---

## ðŸš¨ **Emergency Contacts**

If you encounter issues:
1. Check `docker logs aether-prom` for Prometheus errors
2. Check `docker logs aether-grafana` for Grafana errors
3. Rollback: `git checkout monitoring/ && docker compose restart prometheus grafana`
4. Fallback: Scale to 1 consumer: `docker compose up -d --scale crm-events=1`

---

**End of Deployment Summary**
