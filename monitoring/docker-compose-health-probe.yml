# Docker Compose Integration - Bullet-Proof Health Probe Configuration
# Add this to your monitoring/docker-compose.yml

services:
  # Health probe sidecar (queries Prometheus for consumer health)
  crm-events-health:
    build:
      context: ../pods/customer-ops
      dockerfile: Dockerfile
    container_name: aether-crm-events-health
    command: python health_probe.py
    expose:
      - "9011"  # Internal only (no host port mapping for security)
    environment:
      # Prometheus connection
      - PROMETHEUS_URL=http://prometheus:9090
      - KAFKA_GROUP=crm-events-sse
      
      # Health thresholds
      - SKEW_THRESHOLD=4.0
      - MIN_CONSUMERS=2
      
      # Safety knobs (bullet-proof configuration)
      - PROM_TIMEOUT_MS=1500      # Request timeout
      - PROM_RETRIES=2             # Retry count before failing
      - WINDOW_MINUTES=5           # Windowed queries to prevent flapping
      - BIND_HOST=0.0.0.0          # Bind to all interfaces (use 127.0.0.1 for localhost-only)
    
    restart: unless-stopped
    networks:
      - aether-monitoring
    
    depends_on:
      prometheus:
        condition: service_healthy
    
    # Health probe's own health check (liveness)
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9011/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 5s
  
  # CRM Events consumer (uses health probe for readiness)
  crm-events:
    build: ../pods/customer-ops
    container_name: aether-crm-events
    ports:
      - "9010:9010"
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_TOPIC=aetherlink.events
      - KAFKA_GROUP=crm-events-sse
      - KAFKA_OFFSET_RESET=earliest
    restart: unless-stopped
    networks:
      - aether-monitoring
    depends_on:
      kafka:
        condition: service_started
      crm-events-health:
        condition: service_healthy  # Wait for health probe to be ready
    
    # Consumer's health check (queries sidecar /ready endpoint)
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://crm-events-health:9011/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s  # Allow 30s for consumer to initialize

---

# OPTIONAL: If you want to expose health probe for local testing
# Add this port mapping to crm-events-health:
#   ports:
#     - "9011:9011"

# Then you can test locally:
#   curl http://localhost:9011/ready
#   curl http://localhost:9011/status
#   curl http://localhost:9011/metrics

---

# ALTERNATIVE: If you need to remove container_name for scaling
# (Docker scaling requires auto-generated names)

services:
  crm-events-health:
    # ... (same as above, but remove container_name)
    # container_name: aether-crm-events-health  # <-- REMOVE THIS LINE
  
  crm-events:
    # ... (same as above, but remove container_name)
    # container_name: aether-crm-events  # <-- REMOVE THIS LINE
    
    # Then you can scale:
    # docker compose up -d --scale crm-events=2 --scale crm-events-health=1

---

# KUBERNETES EQUIVALENT

apiVersion: apps/v1
kind: Deployment
metadata:
  name: crm-events-sse
spec:
  replicas: 2
  selector:
    matchLabels:
      app: crm-events-sse
  template:
    metadata:
      labels:
        app: crm-events-sse
    spec:
      containers:
      # Main consumer container
      - name: crm-events
        image: aetherlink/crm-events:latest
        ports:
        - containerPort: 9010
          name: metrics
        env:
        - name: KAFKA_BROKERS
          value: "kafka:9092"
        - name: KAFKA_TOPIC
          value: "aetherlink.events"
        - name: KAFKA_GROUP
          value: "crm-events-sse"
        
        # Liveness: Restart if consumer process is dead
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9010
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness: Use health probe sidecar (checks skew + consumers)
        readinessProbe:
          httpGet:
            path: /ready
            port: 9011  # Health probe sidecar port
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 2
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      
      # Health probe sidecar container
      - name: health-probe
        image: aetherlink/crm-events-health:latest
        ports:
        - containerPort: 9011
          name: health
        - containerPort: 9012
          name: metrics
        env:
        - name: PROMETHEUS_URL
          value: "http://prometheus:9090"
        - name: KAFKA_GROUP
          value: "crm-events-sse"
        - name: SKEW_THRESHOLD
          value: "4.0"
        - name: MIN_CONSUMERS
          value: "2"
        - name: PROM_TIMEOUT_MS
          value: "1500"
        - name: PROM_RETRIES
          value: "2"
        - name: WINDOW_MINUTES
          value: "5"
        - name: BIND_HOST
          value: "0.0.0.0"
        
        # Liveness: Restart if health probe process is dead
        livenessProbe:
          httpGet:
            path: /health
            port: 9011
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

---

# TESTING COMMANDS

# 1. Test health probe locally
cd pods/customer-ops
pip install -r requirements-health.txt
export PROMETHEUS_URL=http://localhost:9090
export KAFKA_GROUP=crm-events-sse
python health_probe.py

# 2. Test endpoints
curl http://localhost:9011/ready    # 200 or 500
curl http://localhost:9011/health   # Always 200
curl http://localhost:9011/status   # Detailed JSON
curl http://localhost:9011/metrics  # Prometheus metrics

# 3. Simulate unhealthy state (skew >4x)
docker stop aether-crm-events
for i in {1..300}; do
  echo '{"Type":"Test","Key":"HOTKEY"}' | docker exec -i kafka rpk topic produce --key HOTKEY aetherlink.events
done
docker start aether-crm-events

# Wait 5min for windowed metric, then check
curl -i http://localhost:9011/ready
# Should return 500 with JSON showing skew >4x

# 4. Watch health status in real-time
watch -n 5 'curl -s http://localhost:9011/status | jq ".checks"'

# 5. Monitor probe quality metrics
curl -s http://localhost:9011/metrics | grep health_probe

# 6. View JSON logs
docker logs crm-events-health --tail 50 | jq

# 7. Check consumer health via probe
docker inspect crm-events --format='{{.State.Health.Status}}'
# Should show: healthy or unhealthy

---

# MONITORING PROBE QUALITY IN GRAFANA

# Add these panels to your dashboard:

# Panel: Health Probe Status
rate(health_probe_checks_total{status="unhealthy"}[5m])

# Panel: Health Check Failures by Type
rate(health_probe_failures_total[5m]) by (check_type, reason)

# Panel: Health Check Duration
histogram_quantile(0.95, rate(health_probe_duration_seconds_bucket[5m]))

# Panel: Current Health Status
health_probe_status  # 1=healthy, 0=unhealthy

# Panel: Skew Ratio (from probe)
health_probe_skew_ratio

# Panel: Consumer Count (from probe)
health_probe_consumer_count

---

# CONFIGURATION PRESETS

# Development (lenient)
SKEW_THRESHOLD=6.0
MIN_CONSUMERS=1
WINDOW_MINUTES=3
PROM_TIMEOUT_MS=2000
PROM_RETRIES=1

# Production (strict)
SKEW_THRESHOLD=4.0
MIN_CONSUMERS=2
WINDOW_MINUTES=5
PROM_TIMEOUT_MS=1500
PROM_RETRIES=2

# High-Volume (very strict)
SKEW_THRESHOLD=3.0
MIN_CONSUMERS=3
WINDOW_MINUTES=10
PROM_TIMEOUT_MS=1000
PROM_RETRIES=3

---

# SECURITY BEST PRACTICES

1. Internal-only binding:
   - Use BIND_HOST=127.0.0.1 if health probe runs on same host as consumer
   - Use expose: instead of ports: in docker-compose (no host exposure)

2. Network isolation:
   - Health probe should only be in monitoring network
   - Don't expose on public networks

3. No secrets in URLs:
   - Use environment variables for all config
   - Don't log query strings containing sensitive data

4. TLS for production:
   - Use PROMETHEUS_URL=https://prometheus:9090 if TLS is enabled
   - Add certificate verification if using internal CA

---

# TROUBLESHOOTING

# Probe returns 500 (unhealthy) but system looks fine:
# - Check window: 5min window may include past issues
# - Reduce WINDOW_MINUTES to 3 for faster recovery signal
# - Check Prometheus: curl http://prometheus:9090/-/healthy

# Probe always returns 200 (healthy) despite issues:
# - Check thresholds: May be too lenient
# - Verify recording rules: kafka:group_skew_ratio and kafka:group_consumer_count must exist
# - Check logs: docker logs crm-events-health | jq

# Probe times out:
# - Increase PROM_TIMEOUT_MS from 1500 to 3000
# - Increase PROM_RETRIES from 2 to 3
# - Check Prometheus responsiveness

# Container flapping (restarting repeatedly):
# - Increase healthcheck.retries from 3 to 5
# - Increase healthcheck.interval from 30s to 60s
# - Check if thresholds are too strict for your traffic pattern

---

# EXIT CODES (for scripting)

# Readiness endpoint:
# - Exit 0 (200 OK): System healthy, ready for traffic
# - Exit 1 (500 Error): Pathological state detected

# Example script usage:
if curl -fsS http://localhost:9011/ready > /dev/null 2>&1; then
    echo "System healthy"
    exit 0
else
    echo "System unhealthy"
    exit 1
fi
